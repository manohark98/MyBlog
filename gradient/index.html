<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Simple OpenSource Blog</title>
  <style>
    /* Global Styles */
    body {
      background-color: floralwhite;
      color: #333;
      font-family: monospace;
      margin: 0;
      padding: 20px;
      line-height: 1.6;
    }
    /* Content Container */
    .container {
      max-width: 800px;
      margin: 0 auto;
      background-color: #fff;
      padding: 30px;
      border-radius: 4px;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
    }
    /* Blog Post Styles */
    h1, h2 {
      margin-bottom: 15px;
    }
    p {
      margin-bottom: 20px;
    }
    h2 {
      margin-top: 30px;
      border-bottom: 1px solid #eee;
      padding-bottom: 5px;
    }
    /* Code Snippet Container */
    .code-container {
      background-color: #f4f4f4;
      padding: 15px;
      border-radius: 4px;
      margin: 20px 0;
      overflow-x: auto;
    }
    /* Screenshot Container */
    .screenshot {
      margin: 20px 0;
      text-align: center;
    }
    .screenshot img {
      max-width: 100%;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 5px;
    }

    /* Footer Styles */
    footer {
      text-align: center;
      margin-top: 40px;
      font-size: 0.9em;
      color: #666;
    }
  </style>
  <link href="style.css" rel="stylesheet" type="text/css" />
</head>
<body>
  <div class="container">
    <header>
      <div class="logo-container">
        <div class="logo neural-network">
          <svg viewBox="0 0 200 200" width="120" height="120">
            <circle cx="50" cy="50" r="8" class="node"/>
            <circle cx="50" cy="100" r="8" class="node"/>
            <circle cx="50" cy="150" r="8" class="node"/>
            <circle cx="150" cy="75" r="8" class="node"/>
            <circle cx="150" cy="125" r="8" class="node"/>
            <circle cx="100" cy="50" r="8" class="node"/>
            <circle cx="100" cy="100" r="8" class="node"/>
            <circle cx="100" cy="150" r="8" class="node"/>
            <line x1="50" y1="50" x2="100" y2="50" class="connection"/>
            <line x1="50" y1="50" x2="100" y2="100" class="connection"/>
            <line x1="50" y1="100" x2="100" y2="50" class="connection"/>
            <line x1="50" y1="100" x2="100" y2="100" class="connection"/>
            <line x1="50" y1="150" x2="100" y2="100" class="connection"/>
            <line x1="50" y1="150" x2="100" y2="150" class="connection"/>
            <line x1="100" y1="50" x2="150" y2="75" class="connection"/>
            <line x1="100" y1="100" x2="150" y2="75" class="connection"/>
            <line x1="100" y1="100" x2="150" y2="125" class="connection"/>
            <line x1="100" y1="150" x2="150" y2="125" class="connection"/>
          </svg>
        </div>
        <div class="logo peacock">
          <img src="Logo_Blog.png" alt="Blog Logo" width="120" height="120">
        </div>
      </div>
    </header>

    <h1 style="text-align: center;">LilGradX - How Gradients Really Work</h1>
    <p>Date: March 23, 2025</p>

    <h2>Introduction</h2>
    <p>Deep learning libraries like PyTorch and TensorFlow make training neural networks look almost magical: you define a few layers, call <code>.backward()</code>, and—poof!—your weights update. But behind the scenes is a beautiful, systematic process called <strong>automatic differentiation</strong>, which hinges on the notion of a <strong>gradient</strong>. In LilGradX we peel back that curtain: we’ll build a tiny <code>Tensor</code> class, track every operation in a computational graph, and manually compute gradients via the chain rule.</p>

    <h2>Why Gradients Matter</h2>
    <ul>
      <li><strong>Optimization:</strong> Gradients tell us which direction in parameter space will reduce our loss function most quickly.</li>
      <li><strong>Learning:</strong> In neural networks, we use gradients to nudge weights incrementally, making the network better at its task.</li>
      <li><strong>Intuition:</strong> Understanding gradients demystifies much of "deep learning magic" and empowers you to debug tricky training dynamics.</li>
    </ul>

    <h2>Our Tiny <code>Tensor</code> Class</h2>
    <p>We’ll implement a minimal <code>Tensor</code> that holds a value, records its parents, and stores a backward function:</p>
    <div class="code-container">
<pre><code>  class Tensor:
  def __init__(self, data, _children=(), _op='', label='', requires_grad=True):
      
      self.data = np.array(data, dtype=np.float32)
      self.requires_grad = requires_grad
      self.grad = np.zeros_like(self.data, dtype=np.float32) if requires_grad else None
      self._backward = lambda: None
      self._prev = set(_children)
</code></pre>
    </div>

    <h2>Building the Computational Graph</h2>
    <p>Consider the simple neuron:</p>
    <div class="code-container">
<pre><code>x      = Tensor(2.0, label="x")
weight = Tensor(5.0, label="w")
bias   = Tensor(1.0, label="b")

y = x * weight      # multiplication
y = y + bias        # addition
</code></pre>
    </div>
    <div class="screenshot">
      <img src="function.svg" alt="Computational graph">
      <p>Figure 1: Each arrow shows data flow (forward) and each node knows how to backpropagate.</p>
    </div>

    <h2>Forward Pass: Computing Values</h2>
    <p><strong>Multiplication:</strong> m = x × w = 2.0 × 5.0 = 10.0</p>
    <p><strong>Addition:</strong> y = m + b = 10.0 + 1.0 = 11.0</p>

    <h2>Backward Pass: Applying the Chain Rule</h2>
    <p>We set <code>y.grad = 1.0</code> and propagate gradients in reverse:</p>
    <p><strong>Step 1: Addition Node</strong><br>
    &thinsp;&thinsp;∂y/∂m = 1 → <code>m.grad += 1 * y.grad</code><br>
    &thinsp;&thinsp;∂y/∂b = 1 → <code>bias.grad += 1 * y.grad</code></p>
    <p><strong>Step 2: Multiplication Node</strong><br>
    &thinsp;&thinsp;∂m/∂x = w = 5.0 → <code>x.grad += w.value * m.grad</code><br>
    &thinsp;&thinsp;∂m/∂w = x = 2.0 → <code>weight.grad += x.value * m.grad</code></p>

    <h2>Putting It All Together</h2>
    <div class="code-container">
<pre><code># Forward
y = x * weight + bias

# Initialize gradient on output
y.grad = 1.0

# Backward
# addition and multiplication nodes propagate grads
</code></pre>
    </div>

    <h2>Why This Matters</h2>
    <ul>
      <li><strong>Transparency:</strong> See exactly how each operation contributes to the final gradient.</li>
      <li><strong>Debugging:</strong> Diagnose exploding/vanishing gradients or incorrect flows.</li>
      <li><strong>Extensibility:</strong> Add new ops by recording inputs and defining backward functions.</li>
    </ul>

    <h2>Next Steps</h2>
    <ul>
      <li>Extend to vector/matrix ops to build full neural networks.</li>
      <li>Visualize gradient flow in deeper architectures.</li>
      <li>Experiment with learning rates and compare with PyTorch’s autograd.</li>
    </ul>

    <h2>Conclusion</h2>
    <p>Automatic differentiation is the engine of modern deep learning. By building LilGradX from scratch, you gain intimate insight into computational graphs, chain-rule bookkeeping, and gradient propagation—no more mystery behind <code>.backward()</code>!</p>

  </div>
  <footer>
    <p>&copy; 2025 My OpenSource Blog - LilGradX. All rights reserved.</p>
  </footer>
</body>
</html>
